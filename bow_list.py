# -*- coding: utf-8 -*-
"""BOW_list

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EUwK5R_4f2jqbnG8F4iTXtdEI8PvuHOC
"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('wordnet')
import pandas as pd
import string
from nltk.corpus import stopwords
nltk.download('stopwords')

def lower_case(text):
    return text.lower()

def remove_punctuation(text):
    return ''.join([char for char in text if char not in string.punctuation])

def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stopwords.words('english')])

def tokenize(text):
    return word_tokenize(text)

def lemmatize(tokens):
    lemmatizer=WordNetLemmatizer()
    return [lemmatizer.lemmatize(token) for token in tokens]

def lemma(word):
  lemmatizer=WordNetLemmatizer()
  return lemmatizer.lemmatize(word)

bow=[]
def preprocessing(list):
  for word in list:
    if word!="  " or word!=" " or word!="/n":
      #print(word)
      lower_case_words=lower_case(word)
      words_without_stopwords=remove_stopwords(lower_case_words)
      words_without_punctuation=remove_punctuation(words_without_stopwords)
      tokenized_words=tokenize(words_without_punctuation)
      lemmatized_words=lemmatize(tokenized_words)
      list.append(lemmatized_words)
  return list

if __name__=="__main__":
  data_set=pd.read_csv("/content/data_rt.csv")

  data_samples=[]
  data_sample=[]
  for index,row in data_set.iterrows():
    if(row['labels']==0):
      row['labels']='n'
    else:
      row['labels']='p'
    data_sample.append([row['reviews'],row['labels']])
    data_samples.append(data_sample)
    data_sample=[]


# for data_sample in data_samples:
#   review=data_sample[0][0]
#   lower_case_review=lower_case(review)
#   review_without_stopwords=remove_stopwords(lower_case_review)
#   review_without_punctuation=remove_punctuation(review_without_stopwords)
#   tokenized_review=tokenize(review_without_punctuation)
#   lemmatized_review=lemmatize(tokenized_review)
#   data_sample.append(lemmatized_review)

#   lemmatized_review_list=data_sample


# print(data_sample[1])


# for data_sample in data_samples:
#   print(data_sample[0][0])            #For fetching the revies in string form


# print (data_samples[0])

raw_bow=[]
for data_sample in data_samples:
  for word in tokenize(data_sample[0][0]):
    raw_bow.append(word)


clean_bow=[]
clean_bow=preprocessing(str(raw_bow))


# freq_dist=nltk.FreqDist(bow)
# top_2000=freq_dist.most_common(2000)


# print(top_2000)

clean_bow=[]
lemmatizer=WordNetLemmatizer()
for word in raw_bow:
  if word not in stopwords.words('english'):
    # print(word)
    lemmatized_word=lemma(word)
    # clean_bow.append(lemmatized_word)
    # print(lemmatized_word)

print(clean_bow[:20])

featurs=nltk.FreqDist(clean_bow)
print(featurs.most_common(2000))

#data_samples=[]
#data_sample[review,label[lemmatized_review]]
#data_sample[0][0]=review
#data_sample[0][1]=label
#data_sample[1]=lemmatized_review

print(raw_bow[:100])

print(clean_bow[:100])





