# -*- coding: utf-8 -*-
"""Feature_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F0Rpr38hUr9v_gjg18yTjL3A9QkP81lX
"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
import pandas as pd
import string
from nltk.corpus import stopwords
nltk.download('stopwords')

def lower_case(text):
    return text.lower()

def remove_punctuation(text):
    return ''.join([char for char in text if char not in string.punctuation])

def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stopwords.words('english')])

def tokenize(text):
    return word_tokenize(text)

def lemmatize(tokens):
    lemmatizer=WordNetLemmatizer()
    return [lemmatizer.lemmatize(token) for token in tokens]

if __name__=="__main__":
  data_set=pd.read_csv("/content/data_rt.csv")

  data_samples=[]
  data_sample=[]
  for index,row in data_set.iterrows():
    data_sample=[]
    if(row['labels']==0):
      row['labels']='n'
    else:
      row['labels']='p'
    data_sample.append(row['reviews'])
    data_sample.append(row['labels'])
    data_samples.append(data_sample)

# for data_sample in data_samples:
#   review=data_sample[0][0]
#   lower_case_review=lower_case(review)
#   review_without_stopwords=remove_stopwords(lower_case_review)
#   review_without_punctuation=remove_punctuation(review_without_stopwords)
#   tokenized_review=tokenize(review_without_punctuation)
#   lemmatized_review=lemmatize(tokenized_review)
#   data_sample.append(lemmatized_review)

#   lemmatized_review_list=data_sample


# print(data_sample[1])


# bow=[]
# for data_sample in data_samples:
#   for word in data_sample[1]:
#     bow.append(word)

# freq_dist=nltk.FreqDist(bow)
# top_2000=freq_dist.most_common(2000)


# print(top_2000)

# feature_set=[]

# for data_sample in data_samples:
# for feature in top_2000:
#   if feature[0] in data_sample[1]:
#     feature_set.append((feature[0],"True"))
#   else:
#     feature_set.append((feature[0],"False"))

# # print(feature_set)

# #If you are interested in visualizing the features that are matched with the data_sample(review) you can run the code given below

# for x in range(2000):
#   if feature_set[x][1]=="True":
#     print(feature_set[x][0])

data_samples_c=data_samples[:]

for data_sample in data_samples:
  review=str(data_sample[0])
  lower_case_review=lower_case(review)
  review_without_stopwords=remove_stopwords(lower_case_review)
  review_without_punctuation=remove_punctuation(review_without_stopwords)
  tokenized_review=tokenize(review_without_punctuation)
  lemmatized_review=lemmatize(tokenized_review)
  data_sample.append(lemmatized_review)

  # lemmatized_review_list=data_sample

data_samples[-1][2]

BOW=[]
for data_sample in data_samples:
  for word in data_sample[2]:
    # print(word)
    BOW.append(word)

freq_dist=nltk.FreqDist(BOW)
top_2000=freq_dist.most_common(2000)

top_2000_c=top_2000[:]

top_2000

feature_set=[]
feature_model=[]
for data_sample in data_samples:
  for feature in top_2000_c:
    if feature[0] in data_sample[2][0]:
      feature_set.append((feature[0],1))
    else:
      feature_set.append((feature[0],0))
    for value in label_data[1]:
      feature_set.append((value))
  feature_model.append(feature_set)
  feature_set=[]

label_data_samples=[]
label_data_sample=[]
for data_sample in data_samples_c:
  label_data_sample.append(data_sample[0])
  label_data_sample.append(data_sample[1])
  label_data_samples.append(label_data_sample)
  label_data_sample=[]

feature_set_c=[]
feature_model_c=[]
for data_sample in data_samples:
  for feature in top_2000_c:
    if feature[0] in data_sample[2][0]:
      feature_set_c.append((feature[0],1))
    else:
      feature_set_c.append((feature[0],0))
    for label_data_sample in label_data_samples:
      feature_set_c.append((label_data_sample[1]))
  feature_model_c.append(feature_set)
  feature_set_c=[]

feature_model_c[-1]

import random
shuffled_data=random.sample(feature_model,len(feature_model))

for label_data_sample in label_data_samples:
  print(label_data_sample)

# for data_sample in data_samples:
#   print(data_sample[3])
len(data_samples)

top_2000

for data_sample in data_samples:
  print(data_sample[2][0])

